Deep Atlas Training Log
Epoch 1/150:
	reg training loss: 11.821082496643067
	reg validation loss: 0.37389451265335083
	seg training loss: 0.5597355484962463
	seg validation loss: 0.6094745397567749
Epoch 2/150:
	reg training loss: 6.6504021167755125
	seg training loss: 0.5137436628341675
Epoch 3/150:
	reg training loss: 4.567505621910096
	seg training loss: 0.5032413363456726
Epoch 4/150:
	reg training loss: 3.5391402721405028
	seg training loss: 0.5013756871223449
Epoch 5/150:
	reg training loss: 2.8909197092056274
	seg training loss: 0.5007793784141541
Epoch 6/150:
	reg training loss: 2.3794663190841674
	reg validation loss: 0.522308087348938
	seg training loss: 0.5001442611217499
	seg validation loss: 0.5000951290130615
Epoch 7/150:
	reg training loss: 2.0016581535339357
	seg training loss: 0.4996967911720276
Epoch 8/150:
	reg training loss: 1.668310534954071
	seg training loss: 0.49840810894966125
Epoch 9/150:
	reg training loss: 1.5610418319702148
	seg training loss: 0.49862489104270935
Epoch 10/150:
	reg training loss: 1.3587117075920105
	seg training loss: 0.49725581407547
Epoch 11/150:
	reg training loss: 1.147775685787201
	reg validation loss: 0.49597218036651614
	seg training loss: 0.4906475007534027
	seg validation loss: 0.4911479353904724
Epoch 12/150:
	reg training loss: 0.9887164473533631
	seg training loss: 0.486864709854126
Epoch 13/150:
	reg training loss: 0.9129013359546662
	seg training loss: 0.47430355548858644
Epoch 14/150:
	reg training loss: 0.8470853924751282
	seg training loss: 0.4575150370597839
Epoch 15/150:
	reg training loss: 0.6304636120796203
	seg training loss: 0.45608034133911135
Epoch 16/150:
	reg training loss: 0.6495123952627182
	reg validation loss: 0.4035495609045029
	seg training loss: 0.4368858814239502
	seg validation loss: 0.5302151441574097
Epoch 17/150:
	reg training loss: 0.5760160654783248
	seg training loss: 0.4357334911823273
Epoch 18/150:
	reg training loss: 0.5347216695547103
	seg training loss: 0.3983084559440613
Epoch 19/150:
	reg training loss: 0.5073233485221863
	seg training loss: 0.3845296025276184
Epoch 20/150:
	reg training loss: 0.4664753437042236
	seg training loss: 0.3979134202003479
Epoch 21/150:
	reg training loss: 0.3759556323289871
	reg validation loss: 0.3557498097419739
	seg training loss: 0.37429308891296387
	seg validation loss: 0.4269775152206421
Epoch 22/150:
	reg training loss: 0.3640876829624176
	seg training loss: 0.37723526954650877
Epoch 23/150:
	reg training loss: 0.3392740964889526
	seg training loss: 0.34355148673057556
Epoch 24/150:
	reg training loss: 0.29378232955932615
	seg training loss: 0.3746404528617859
Epoch 25/150:
	reg training loss: 0.2828072488307953
	seg training loss: 0.3297532230615616
Epoch 26/150:
	reg training loss: 0.28436259031295774
	reg validation loss: 0.30200499296188354
	seg training loss: 0.334561425447464
	seg validation loss: 0.29631733894348145
Epoch 27/150:
	reg training loss: 0.3075242906808853
	seg training loss: 0.3282738924026489
Epoch 28/150:
	reg training loss: 0.22323549389839173
	seg training loss: 0.29739847779273987
Epoch 29/150:
	reg training loss: 0.23728162050247192
	seg training loss: 0.3552877426147461
Epoch 30/150:
	reg training loss: 0.20461379885673522
	seg training loss: 0.2977589249610901
Epoch 31/150:
	reg training loss: 0.1928160548210144
	reg validation loss: 0.2611283838748932
	seg training loss: 0.3787256181240082
	seg validation loss: 0.2877733111381531
Epoch 32/150:
	reg training loss: 0.2135212242603302
	seg training loss: 0.2828103244304657
Epoch 33/150:
	reg training loss: 0.19590576291084288
	seg training loss: 0.3033428698778152
Epoch 34/150:
	reg training loss: 0.19140681624412537
	seg training loss: 0.27385866045951845
Epoch 35/150:
	reg training loss: 0.16991758942604065
	seg training loss: 0.2762846529483795
Epoch 36/150:
	reg training loss: 0.16104696989059447
	reg validation loss: 0.29244559407234194
	seg training loss: 0.3124780058860779
	seg validation loss: 0.3415263593196869
Epoch 37/150:
	reg training loss: 0.18551079630851747
	seg training loss: 0.29467666149139404
Epoch 38/150:
	reg training loss: 0.15942039489746093
	seg training loss: 0.3284853398799896
Epoch 39/150:
	reg training loss: 0.1474219501018524
	seg training loss: 0.34519295692443847
Epoch 40/150:
	reg training loss: 0.13529703617095948
	seg training loss: 0.291433709859848
Epoch 41/150:
	reg training loss: 0.15530967116355895
	reg validation loss: 0.29838513731956484
	seg training loss: 0.2825896680355072
	seg validation loss: 0.345089316368103
Epoch 42/150:
	reg training loss: 0.1760378360748291
	seg training loss: 0.3347180187702179
Epoch 43/150:
	reg training loss: 0.21043664813041688
	seg training loss: 0.2670065701007843
Epoch 44/150:
	reg training loss: 0.172269207239151
	seg training loss: 0.33537480533123015
Epoch 45/150:
	reg training loss: 0.1514192044734955
	seg training loss: 0.33016109466552734
Epoch 46/150:
	reg training loss: 0.12319523692131043
	reg validation loss: 0.311724978685379
	seg training loss: 0.27000177800655367
	seg validation loss: 0.33103013038635254
Epoch 47/150:
	reg training loss: 0.1216749370098114
	seg training loss: 0.2599552243947983
Epoch 48/150:
	reg training loss: 0.14252822995185851
	seg training loss: 0.27502892911434174
Epoch 49/150:
	reg training loss: 0.14947011470794677
	seg training loss: 0.26649496555328367
Epoch 50/150:
	reg training loss: 0.08678692579269409
	seg training loss: 0.2881341904401779
Epoch 51/150:
	reg training loss: 0.21761186718940734
	reg validation loss: 0.295854252576828
	seg training loss: 0.26804264187812804
	seg validation loss: 0.30262619256973267
Epoch 52/150:
	reg training loss: 0.1505505383014679
	seg training loss: 0.2526878356933594
Epoch 53/150:
	reg training loss: 0.1400753378868103
	seg training loss: 0.24621081054210664
Epoch 54/150:
	reg training loss: 0.16132696270942687
	seg training loss: 0.2821154326200485
Epoch 55/150:
	reg training loss: 0.09471744894981385
	seg training loss: 0.2687530696392059
Epoch 56/150:
	reg training loss: 0.15414805412292482
	reg validation loss: 0.3024827241897583
	seg training loss: 0.2961694777011871
	seg validation loss: 0.30391430854797363
Epoch 57/150:
	reg training loss: 0.10800511837005615
	seg training loss: 0.289970389008522
Epoch 58/150:
	reg training loss: 0.1560879647731781
	seg training loss: 0.24613168835639954
Epoch 59/150:
	reg training loss: 0.10751190185546874
	seg training loss: 0.24189504086971284
Epoch 60/150:
	reg training loss: 0.09544839262962342
	seg training loss: 0.26334519386291505
Epoch 61/150:
	reg training loss: 0.0979136049747467
	reg validation loss: 0.2950019180774689
	seg training loss: 0.22028428316116333
	seg validation loss: 0.2755584716796875
Epoch 62/150:
	reg training loss: 0.12603519558906556
	seg training loss: 0.23675723969936371
Epoch 63/150:
	reg training loss: 0.12293066382408142
	seg training loss: 0.23771210610866547
Epoch 64/150:
	reg training loss: 0.14060842990875244
	seg training loss: 0.2671383798122406
Epoch 65/150:
	reg training loss: 0.0878626823425293
	seg training loss: 0.22713165283203124
Epoch 66/150:
	reg training loss: 0.1267039358615875
	reg validation loss: 0.30810909271240233
	seg training loss: 0.2150157004594803
	seg validation loss: 0.28999578952789307
Epoch 67/150:
	reg training loss: 0.14586974382400514
	seg training loss: 0.25333209335803986
Epoch 68/150:
	reg training loss: 0.1389612853527069
	seg training loss: 0.2535517692565918
Epoch 69/150:
	reg training loss: 0.10728780031204224
	seg training loss: 0.2793023526668549
Epoch 70/150:
	reg training loss: 0.11726493239402772
	seg training loss: 0.25775950253009794
Epoch 71/150:
	reg training loss: 0.10487593412399292
	reg validation loss: 0.30286029577255247
	seg training loss: 0.20050616264343263
	seg validation loss: 0.26846811175346375
Epoch 72/150:
	reg training loss: 0.11433739066123963
	seg training loss: 0.21624718606472015
Epoch 73/150:
	reg training loss: 0.1613703191280365
	seg training loss: 0.21542528569698333
Epoch 74/150:
	reg training loss: 0.07930721640586853
	seg training loss: 0.22874736785888672
Epoch 75/150:
	reg training loss: 0.08886737823486328
	seg training loss: 0.2200069934129715
Epoch 76/150:
	reg training loss: 0.09175273776054382
	reg validation loss: 0.2907645583152771
	seg training loss: 0.23599567860364914
	seg validation loss: 0.26952481269836426
Epoch 77/150:
	reg training loss: 0.07685388326644897
	seg training loss: 0.30224021077156066
Epoch 78/150:
	reg training loss: 0.08563528060913086
	seg training loss: 0.19496178030967712
Epoch 79/150:
	reg training loss: 0.08979331851005554
	seg training loss: 0.23003870844841004
Epoch 80/150:
	reg training loss: 0.14668931365013121
	seg training loss: 0.15787797123193742
Epoch 81/150:
	reg training loss: 0.08079328536987304
	reg validation loss: 0.30960309505462646
	seg training loss: 0.16593604832887648
	seg validation loss: 0.2621419131755829
Epoch 82/150:
	reg training loss: 0.062292033433914186
	seg training loss: 0.1986004814505577
Epoch 83/150:
	reg training loss: 0.08164514303207397
	seg training loss: 0.19187607169151305
Epoch 84/150:
	reg training loss: 0.07091431617736817
	seg training loss: 0.2382894992828369
Epoch 85/150:
	reg training loss: 0.16154407262802123
	seg training loss: 0.2095881223678589
Epoch 86/150:
	reg training loss: 0.08254963159561157
	reg validation loss: 0.2923672914505005
	seg training loss: 0.20030573904514312
	seg validation loss: 0.28055524826049805
Epoch 87/150:
	reg training loss: 0.15470366477966307
	seg training loss: 0.2268322765827179
Epoch 88/150:
	reg training loss: 0.1073101282119751
	seg training loss: 0.17905664443969727
Epoch 89/150:
	reg training loss: 0.08986663222312927
	seg training loss: 0.18232050240039827
Epoch 90/150:
	reg training loss: 0.08085123300552369
	seg training loss: 0.13598624467849732
Epoch 91/150:
	reg training loss: 0.1049730122089386
	reg validation loss: 0.2945237398147583
	seg training loss: 0.21763037741184235
	seg validation loss: 0.2914091944694519
Epoch 92/150:
	reg training loss: 0.14983614683151245
	seg training loss: 0.20359588861465455
Epoch 93/150:
	reg training loss: 0.08720793128013611
	seg training loss: 0.1818316772580147
Epoch 94/150:
	reg training loss: 0.08997194170951843
	seg training loss: 0.2511433094739914
Epoch 95/150:
	reg training loss: 0.07198815941810607
	seg training loss: 0.1950538158416748
Epoch 96/150:
	reg training loss: 0.09011232852935791
	reg validation loss: 0.3002076268196106
	seg training loss: 0.2171761229634285
	seg validation loss: 0.275573194026947
Epoch 97/150:
	reg training loss: 0.08430697321891785
	seg training loss: 0.13103730380535125
Epoch 98/150:
	reg training loss: 0.09184492826461792
	seg training loss: 0.18973634392023087
Epoch 99/150:
	reg training loss: 0.10806086659431458
	seg training loss: 0.1822121486067772
Epoch 100/150:
	reg training loss: 0.06901057958602905
	seg training loss: 0.20531646013259888
Epoch 101/150:
	reg training loss: 0.12045143842697144
	reg validation loss: 0.2907007038593292
	seg training loss: 0.09885678589344024
	seg validation loss: 0.25663310289382935
Epoch 102/150:
	reg training loss: 0.08232075572013856
	seg training loss: 0.15464863926172256
Epoch 103/150:
	reg training loss: 0.035024821758270264
	seg training loss: 0.19866269677877427
Epoch 104/150:
	reg training loss: 0.07489314079284667
	seg training loss: 0.22305773496627807
Epoch 105/150:
	reg training loss: 0.059893786907196045
	seg training loss: 0.22456680685281755
Epoch 106/150:
	reg training loss: 0.10134118199348449
	reg validation loss: 0.2916110932826996
	seg training loss: 0.14105368703603743
	seg validation loss: 0.28060662746429443
Epoch 107/150:
	reg training loss: 0.05982571244239807
	seg training loss: 0.14337918758392335
Epoch 108/150:
	reg training loss: 0.06198419332504272
	seg training loss: 0.16301681399345397
Epoch 109/150:
	reg training loss: 0.10277495384216309
	seg training loss: 0.16241898834705354
Epoch 110/150:
	reg training loss: 0.11861439943313598
	seg training loss: 0.20744090676307678
Epoch 111/150:
	reg training loss: 0.046224570274353026
	reg validation loss: 0.28502326011657714
	seg training loss: 0.14511395394802093
	seg validation loss: 0.26549872756004333
Epoch 112/150:
	reg training loss: 0.07679620385169983
	seg training loss: 0.11356136798858643
Epoch 113/150:
	reg training loss: 0.07011784911155701
	seg training loss: 0.20253922790288925
Epoch 114/150:
	reg training loss: 0.015090751647949218
	seg training loss: 0.2404465988278389
Epoch 115/150:
	reg training loss: 0.08956599235534668
	seg training loss: 0.11676689237356186
Epoch 116/150:
	reg training loss: 0.09242704510688782
	reg validation loss: 0.27108725905418396
	seg training loss: 0.11978376507759095
	seg validation loss: 0.33262014389038086
Epoch 117/150:
	reg training loss: 0.06803345680236816
	seg training loss: 0.15019257813692094
Epoch 118/150:
	reg training loss: 0.05623839497566223
	seg training loss: 0.1964965283870697
Epoch 119/150:
	reg training loss: 0.11808516383171082
	seg training loss: 0.19398911148309708
Epoch 120/150:
	reg training loss: 0.04180136322975159
	seg training loss: 0.15387391448020935
Epoch 121/150:
	reg training loss: 0.019174742698669433
	reg validation loss: 0.2966694116592407
	seg training loss: 0.19064362794160844
	seg validation loss: 0.3111326992511749
Epoch 122/150:
	reg training loss: 0.058362793922424314
	seg training loss: 0.14903774857521057
Epoch 123/150:
	reg training loss: 0.0866361677646637
	seg training loss: 0.10538634508848191
Epoch 124/150:
	reg training loss: 0.014072263240814209
	seg training loss: 0.17474961280822754
Epoch 125/150:
	reg training loss: -0.002625805139541626
	seg training loss: 0.08333210498094559
Epoch 126/150:
	reg training loss: 0.08028135299682618
	reg validation loss: 0.2867518663406372
	seg training loss: 0.1867377758026123
	seg validation loss: 0.2880055904388428
Epoch 127/150:
	reg training loss: -0.018426847457885743
	seg training loss: 0.19549625217914582
Epoch 128/150:
	reg training loss: 0.08934178352355956
	seg training loss: 0.16485425382852553
Epoch 129/150:
	reg training loss: 0.08516538143157959
	seg training loss: 0.1909736767411232
Epoch 130/150:
	reg training loss: 0.055179446935653687
	seg training loss: 0.12937012165784836
Epoch 131/150:
	reg training loss: -0.007635813951492309
	reg validation loss: 0.27655012011528013
	seg training loss: 0.25629244446754457
	seg validation loss: 0.28615713119506836
Epoch 132/150:
	reg training loss: 0.030621832609176634
	seg training loss: 0.16448016166687013
Epoch 133/150:
	reg training loss: 0.07679932117462158
	seg training loss: 0.1657615691423416
Epoch 134/150:
	reg training loss: 0.06709120869636535
	seg training loss: 0.12327997833490371
Epoch 135/150:
	reg training loss: 0.05963452458381653
	seg training loss: 0.14747190922498704
Epoch 136/150:
	reg training loss: 0.04174093008041382
	reg validation loss: 0.29400997757911684
	seg training loss: 0.1851422131061554
	seg validation loss: 0.287497878074646
Epoch 137/150:
	reg training loss: 0.05581270456314087
	seg training loss: 0.14275818467140197
Epoch 138/150:
	reg training loss: -0.0204273521900177
	seg training loss: 0.14410132318735122
Epoch 139/150:
	reg training loss: -0.0016712307929992675
	seg training loss: 0.1824228748679161
Epoch 140/150:
	reg training loss: 0.07324715256690979
	seg training loss: 0.16538342833518982
Epoch 141/150:
	reg training loss: 0.09579830169677735
	reg validation loss: 0.29195292592048644
	seg training loss: 0.1217405453324318
	seg validation loss: 0.2963874638080597
Epoch 142/150:
	reg training loss: -0.021729522943496705
	seg training loss: 0.14555817544460298
Epoch 143/150:
	reg training loss: 0.09731371402740478
	seg training loss: 0.13734984695911406
Epoch 144/150:
	reg training loss: 0.04128246307373047
	seg training loss: 0.1633056193590164
Epoch 145/150:
	reg training loss: 0.05253924131393432
	seg training loss: 0.16074871271848679
Epoch 146/150:
	reg training loss: 0.014925235509872436
	reg validation loss: 0.2746258616447449
	seg training loss: 0.12361070215702057
	seg validation loss: 0.29216504096984863
Epoch 147/150:
	reg training loss: 0.011304157972335815
	seg training loss: 0.12292512059211731
Epoch 148/150:
	reg training loss: 0.018293464183807374
	seg training loss: 0.1360162764787674
Epoch 149/150:
	reg training loss: 0.04139983654022217
	seg training loss: 0.20196976661682128
Epoch 150/150:
	reg training loss: 0.04399541616439819
	seg training loss: 0.12018335908651352


Best reg_net validation loss: 0.2611283838748932
Best seg_net validation loss: 0.25663310289382935