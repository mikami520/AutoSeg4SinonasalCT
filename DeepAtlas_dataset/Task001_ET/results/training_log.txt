Deep Atlas Training Log
Epoch 1/100:
	reg training loss: 0.22646218538284302
	reg validation loss: 0.3565535843372345
	seg training loss: 0.559162175655365
	seg validation loss: 0.6063309907913208
Epoch 2/100:
	reg training loss: 0.29486594200134275
	seg training loss: 0.5130922436714173
Epoch 3/100:
	reg training loss: 0.24525797367095947
	seg training loss: 0.5030768752098084
Epoch 4/100:
	reg training loss: 0.22130959033966063
	seg training loss: 0.5013250231742858
Epoch 5/100:
	reg training loss: 0.2491077721118927
	seg training loss: 0.5007651567459106
Epoch 6/100:
	reg training loss: 0.17559101581573486
	reg validation loss: 0.3237760663032532
	seg training loss: 0.5001822292804718
	seg validation loss: 0.5001816749572754
Epoch 7/100:
	reg training loss: 0.17632485628128053
	seg training loss: 0.4997895896434784
Epoch 8/100:
	reg training loss: 0.17081038355827333
	seg training loss: 0.49909292459487914
Epoch 9/100:
	reg training loss: 0.23913756012916565
	seg training loss: 0.4968187570571899
Epoch 10/100:
	reg training loss: 0.20899290442466736
	seg training loss: 0.49585763216018675
Epoch 11/100:
	reg training loss: 0.1596650242805481
	reg validation loss: 0.3008125424385071
	seg training loss: 0.4852439880371094
	seg validation loss: 0.49334466457366943
Epoch 12/100:
	reg training loss: 0.16528206467628478
	seg training loss: 0.48319963812828065
Epoch 13/100:
	reg training loss: 0.17055275440216064
	seg training loss: 0.4755765974521637
Epoch 14/100:
	reg training loss: 0.1929769277572632
	seg training loss: 0.4580399036407471
Epoch 15/100:
	reg training loss: 0.11917988657951355
	seg training loss: 0.46624414920806884
Epoch 16/100:
	reg training loss: 0.1646326243877411
	reg validation loss: 0.2981757640838623
	seg training loss: 0.45536319017410276
	seg validation loss: 0.5922337174415588
Epoch 17/100:
	reg training loss: 0.18617602586746215
	seg training loss: 0.4293132841587067
Epoch 18/100:
	reg training loss: 0.16189767122268678
	seg training loss: 0.3873481035232544
Epoch 19/100:
	reg training loss: 0.17195234298706055
	seg training loss: 0.4155241012573242
Epoch 20/100:
	reg training loss: 0.18420616388320923
	seg training loss: 0.40800262689590455
Epoch 21/100:
	reg training loss: 0.15259308815002443
	reg validation loss: 0.28570306301116943
	seg training loss: 0.39594185948371885
	seg validation loss: 0.3528044819831848
Epoch 22/100:
	reg training loss: 0.13880470991134644
	seg training loss: 0.39027771949768064
Epoch 23/100:
	reg training loss: 0.14880407452583314
	seg training loss: 0.3345359981060028
Epoch 24/100:
	reg training loss: 0.14894471764564515
	seg training loss: 0.40019023418426514
Epoch 25/100:
	reg training loss: 0.11835289597511292
	seg training loss: 0.3174963116645813
Epoch 26/100:
	reg training loss: 0.16842503547668458
	reg validation loss: 0.2835273504257202
	seg training loss: 0.3259855151176453
	seg validation loss: 0.4095471501350403
Epoch 27/100:
	reg training loss: 0.196987122297287
	seg training loss: 0.34199094772338867
Epoch 28/100:
	reg training loss: 0.09684245586395264
	seg training loss: 0.3018005073070526
Epoch 29/100:
	reg training loss: 0.1530754566192627
	seg training loss: 0.35682175755500795
Epoch 30/100:
	reg training loss: 0.1138855755329132
	seg training loss: 0.2873132944107056
Epoch 31/100:
	reg training loss: 0.14759965538978576
	reg validation loss: 0.2901102304458618
	seg training loss: 0.37866680026054383
	seg validation loss: 0.30496343970298767
Epoch 32/100:
	reg training loss: 0.16200417280197144
	seg training loss: 0.3029345631599426
Epoch 33/100:
	reg training loss: 0.12098777294158936
	seg training loss: 0.3168498188257217
Epoch 34/100:
	reg training loss: 0.1455795168876648
	seg training loss: 0.2750587582588196
Epoch 35/100:
	reg training loss: 0.10845769047737122
	seg training loss: 0.28304417729377745
Epoch 36/100:
	reg training loss: 0.12160090804100036
	reg validation loss: 0.23922752737998962
	seg training loss: 0.3139928996562958
	seg validation loss: 0.3107084333896637
Epoch 37/100:
	reg training loss: 0.13864879608154296
	seg training loss: 0.2847361147403717
Epoch 38/100:
	reg training loss: 0.11071184873580933
	seg training loss: 0.3191102623939514
Epoch 39/100:
	reg training loss: 0.12461004853248596
	seg training loss: 0.32865667939186094
Epoch 40/100:
	reg training loss: 0.12001429796218872
	seg training loss: 0.27966459691524503
Epoch 41/100:
	reg training loss: 0.11445736885070801
	reg validation loss: 0.28218547701835633
	seg training loss: 0.2627426713705063
	seg validation loss: 0.3338777422904968
Epoch 42/100:
	reg training loss: 0.1459998905658722
	seg training loss: 0.3281967043876648
Epoch 43/100:
	reg training loss: 0.17881791591644286
	seg training loss: 0.2502796590328217
Epoch 44/100:
	reg training loss: 0.13755425214767455
	seg training loss: 0.32705456018447876
Epoch 45/100:
	reg training loss: 0.1209954559803009
	seg training loss: 0.32304039001464846
Epoch 46/100:
	reg training loss: 0.11446394324302674
	reg validation loss: 0.2802695453166962
	seg training loss: 0.2674622178077698
	seg validation loss: 0.30868157744407654
Epoch 47/100:
	reg training loss: 0.09347756505012512
	seg training loss: 0.2480789303779602
Epoch 48/100:
	reg training loss: 0.11329384446144104
	seg training loss: 0.27543808817863463
Epoch 49/100:
	reg training loss: 0.12321921586990356
	seg training loss: 0.2655754327774048
Epoch 50/100:
	reg training loss: 0.0760215163230896
	seg training loss: 0.27785785794258117
Epoch 51/100:
	reg training loss: 0.20240457653999328
	reg validation loss: 0.27113577723503113
	seg training loss: 0.2612622737884521
	seg validation loss: 0.29637056589126587
Epoch 52/100:
	reg training loss: 0.12938716411590576
	seg training loss: 0.2523492693901062
Epoch 53/100:
	reg training loss: 0.13088210225105285
	seg training loss: 0.2483266830444336
Epoch 54/100:
	reg training loss: 0.14373546838760376
	seg training loss: 0.27203302681446073
Epoch 55/100:
	reg training loss: 0.09900534749031067
	seg training loss: 0.25665300488471987
Epoch 56/100:
	reg training loss: 0.13241128325462342
	reg validation loss: 0.28688706159591676
	seg training loss: 0.28908146023750303
	seg validation loss: 0.29110002517700195
Epoch 57/100:
	reg training loss: 0.09430865049362183
	seg training loss: 0.27517967522144315
Epoch 58/100:
	reg training loss: 0.1444678783416748
	seg training loss: 0.22603734135627745
Epoch 59/100:
	reg training loss: 0.0937464714050293
	seg training loss: 0.23508076667785643
Epoch 60/100:
	reg training loss: 0.08519882559776307
	seg training loss: 0.2616238951683044
Epoch 61/100:
	reg training loss: 0.09269077777862549
	reg validation loss: 0.28979557752609253
	seg training loss: 0.22044389545917512
	seg validation loss: 0.3643500804901123
Epoch 62/100:
	reg training loss: 0.132667875289917
	seg training loss: 0.2349220484495163
Epoch 63/100:
	reg training loss: 0.1216012954711914
	seg training loss: 0.23395915031433107
Epoch 64/100:
	reg training loss: 0.14054632782936097
	seg training loss: 0.2607313632965088
Epoch 65/100:
	reg training loss: 0.07056048512458801
	seg training loss: 0.219479101896286
Epoch 66/100:
	reg training loss: 0.11493363380432128
	reg validation loss: 0.2744666516780853
	seg training loss: 0.2070322871208191
	seg validation loss: 0.36664074659347534
Epoch 67/100:
	reg training loss: 0.13341674208641052
	seg training loss: 0.253905725479126
Epoch 68/100:
	reg training loss: 0.11312112212181091
	seg training loss: 0.25057036280632017
Epoch 69/100:
	reg training loss: 0.10451217293739319
	seg training loss: 0.27698780298233033
Epoch 70/100:
	reg training loss: 0.1041445016860962
	seg training loss: 0.25921509265899656
Epoch 71/100:
	reg training loss: 0.08364078402519226
	reg validation loss: 0.2781798839569092
	seg training loss: 0.18561892211437225
	seg validation loss: 0.29348304867744446
Epoch 72/100:
	reg training loss: 0.09921018481254577
	seg training loss: 0.21649231016635895
Epoch 73/100:
	reg training loss: 0.12760435938835143
	seg training loss: 0.2193385899066925
Epoch 74/100:
	reg training loss: 0.060314327478408813
	seg training loss: 0.2285832330584526
Epoch 75/100:
	reg training loss: 0.09333699941635132
	seg training loss: 0.2211754024028778
Epoch 76/100:
	reg training loss: 0.078071928024292
	reg validation loss: 0.2719071269035339
	seg training loss: 0.23430490642786025
	seg validation loss: 0.29596656560897827
Epoch 77/100:
	reg training loss: 0.05177584886550903
	seg training loss: 0.30461653470993044
Epoch 78/100:
	reg training loss: 0.0742658257484436
	seg training loss: 0.1882425829768181
Epoch 79/100:
	reg training loss: 0.08560987710952758
	seg training loss: 0.23191708624362944
Epoch 80/100:
	reg training loss: 0.13203709721565246
	seg training loss: 0.16292519271373748
Epoch 81/100:
	reg training loss: 0.07805619835853576
	reg validation loss: 0.2741308927536011
	seg training loss: 0.17309835851192473
	seg validation loss: 0.28449440002441406
Epoch 82/100:
	reg training loss: 0.04573346376419067
	seg training loss: 0.20239032208919525
Epoch 83/100:
	reg training loss: 0.08696445822715759
	seg training loss: 0.20023352801799774
Epoch 84/100:
	reg training loss: 0.08225492238998414
	seg training loss: 0.24452903270721435
Epoch 85/100:
	reg training loss: 0.15555346012115479
	seg training loss: 0.20900498032569886
Epoch 86/100:
	reg training loss: 0.08152985572814941
	reg validation loss: 0.2726996183395386
	seg training loss: 0.20052351355552672
	seg validation loss: 0.27712997794151306
Epoch 87/100:
	reg training loss: 0.161283677816391
	seg training loss: 0.22641471028327942
Epoch 88/100:
	reg training loss: 0.12036670446395874
	seg training loss: 0.18003752827644348
Epoch 89/100:
	reg training loss: 0.08597907423973083
	seg training loss: 0.18364396542310715
Epoch 90/100:
	reg training loss: 0.0448895514011383
	seg training loss: 0.13381955325603484
Epoch 91/100:
	reg training loss: 0.09179027676582337
	reg validation loss: 0.2872685074806213
	seg training loss: 0.22036462724208833
	seg validation loss: 0.2947162091732025
Epoch 92/100:
	reg training loss: 0.1477696716785431
	seg training loss: 0.20035581439733505
Epoch 93/100:
	reg training loss: 0.08107761740684509
	seg training loss: 0.18164024800062178
Epoch 94/100:
	reg training loss: 0.09574552774429321
	seg training loss: 0.25627546906471255
Epoch 95/100:
	reg training loss: 0.1005835235118866
	seg training loss: 0.19318093061447145
Epoch 96/100:
	reg training loss: 0.06847572922706605
	reg validation loss: 0.2823968052864075
	seg training loss: 0.22782818973064423
	seg validation loss: 0.2720978558063507
Epoch 97/100:
	reg training loss: 0.07331880331039428
	seg training loss: 0.1334550127387047
Epoch 98/100:
	reg training loss: 0.1218350112438202
	seg training loss: 0.19116145074367524
Epoch 99/100:
	reg training loss: 0.11947042346000672
	seg training loss: 0.19290745556354522
Epoch 100/100:
	reg training loss: 0.06209324598312378
	seg training loss: 0.2086450755596161


Best reg_net validation loss: 0.23922752737998962
Best seg_net validation loss: 0.2720978558063507